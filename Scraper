from bs4 import BeautifulSoup
import requests
import time
import csv
from datetime import datetime
import random
import pandas as pd
import regex as re

# TODO Create separate table for genres. That table can be manipulated with pandas to get counts

# ACCESSES HTML CODE
# TooManyRequests error required me to find headers in devtools->network->click top name of waterfall->request headers
headers = {  # infinite redirects to MetaCritic without headers - TODO don't actually need all these?
    # "Authority": "www.metacritic.com",
    # "Method": "GET",
    # "Path": "/",
    # "Scheme": "https",
    # "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,applic"
    #           "ation/signed-exchange;v=b3;q=0.7",
    # "Accept-Encoding": "gzip, deflate, br",
    # "Accept-Language": "en-US,en;q=0.9",
    # "Cache-Control": "max-age=0",
    # "Cookie": "Geo={%22region%22:%22MA%22%2C%22city%22:%22malden%22%2C%22country_name%22:%22united states%22%2C%22coun"
    #           "try%22:%22US%22%2C%22continent%22:%22NA%22}; mc_s_s=c_2; usprivacy=1YNY; wikia_beacon_id=oPVM8t5roi; _"
    #           "cb=DqhqFwCbS4NVC8d-Ad; _li_dcdm_c=.metacritic.com; _lc2_fpi=9c17cb2c9960--01h17md63yaryzn6zrvdmtk6ng; "
    #           "__utmc=15671338; __utmz=15671338.1684957207.1.1.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=("
    #           "not%20provided); _au_1d=AU1D-0100-001684957207-W9TTVVM4-TTWJ; __li_idex_cache_e30={%22unifiedId%22:%22"
    #           "RixLMmIrzI7izxRHyWvjsivJLLtX9A5tRpZ0Gw%22}; _ga=GA1.2.1186384487.1684957207; _gid=GA1.2.1096701357.168"
    #           "4957207; _pbjs_userid_consent_data=3524755945110770; _au_last_seen_pixels=eyJhcG4iOjE2ODQ5NTcyMDcsInR0"
    #           "ZCI6MTY4NDk1NzIwNywicHViIjoxNjg0OTU3MjA3LCJhZHgiOjE2ODQ5NTcyMDcsInJ1YiI6MTY4NDk1NzIwNywidGFwYWQiOjE2OD"
    #           "Q5NTcyMDcsImdvbyI6MTY4NDk1NzIwNywib3BlbngiOjE2ODQ5NTcyMDcsIm1lZGlhbWF0aCI6MTY4NDk1NzIwNywiYWRvIjoxNjg0"
    #           "OTU3MjEwLCJ1bnJ1bHkiOjE2ODQ5NTcyMTAsInNvbiI6MTY4NDk1NzIxMCwiX2ZhbmRvbS1jb20iOjE2ODQ5NTcyMTB9; ctk=NjQ"
    #           "2ZTY4MTE0OTY0NmQwOGRiYjcyYWE3NWUzOA%3D%3D; OneTrustWPCCPAGoogleOptOut=false; __gpi=UID=00000c322449115"
    #           "8:T=1684957214:RT=1684957214:S=ALNI_MZn6pjwyZmszcVWTw8EQEM0vif91A; _cc_id=51c4835ea3ac921781d540d60421"
    #           "6aa3; panoramaId_expiry=1685562016511; panoramaId=92a9438e23ccd7d87ebff19b346416d539381ac34d0b7c640081"
    #           "8e4c1653262c; panoramaIdType=panoIndiv; __gads=ID=9465a27a628a06a5:T=1684957214:RT=1684957319:S=ALNI_M"
    #           "ZLeadqhhpp7bSWUFkne-Ibpw42qA; tmpid=168495747233251; cdn_device=desktop; country_code=us; auth.strateg"
    #           "y=local; sessionId=457ea99a-46fd-4d00-9c1d-950410822122; _cb_svref=null; __utma=15671338.1186384487.16"
    #           "84957207.1684957207.1684961433.2; _BB.bs={%22session%22:%22h%22%2C%22subses%22:%222%22}; __li_idex_cac"
    #           "he_eyJxZiI6IjAuMyIsInJlc29sdmUiOiJzaGExIn0={}; __li_idex_cache_eyJxZiI6IjAuMyIsInJlc29sdmUiOiJtZDUifQ="
    #           "{}; __li_idex_cache_eyJxZiI6IjAuMyIsInJlc29sdmUiOiJzaGEyIn0={}; metapv=20; pvNumber=2; pvNumberGlobal="
    #           "2; OptanonConsent=isGpcEnabled=0&datestamp=Wed+May+24+2023+16%3A52%3A08+GMT-0400+(Eastern+Daylight+Tim"
    #           "e)&version=202303.2.0&browserGpcFlag=0&isIABGlobal=false&hosts=&landingPath=NotLandingPage&groups=C000"
    #           "1%3A1%2CC0003%3A1%2CC0002%3A1%2CC0004%3A1%2CC0005%3A1&AwaitingReconsent=false; _chartbeat2=.1684957206"
    #           "600.1684961528887.1.By3slhDjvCDMDE-M-OfHqDBk8A7m.2; __utmb=15671338.2.10.1684961433; _BB.d={%22ttag%22"
    #           ":%22%22%2C%22ftag%22:%22%22%2C%22pv%22:%222%22}; amp_b9a939=xME4gyoot_9l-TSE-TxdKJ...1h17oe5h1.1h17oh3"
    #           "nr.p.1.q; cto_bundle=2N66Rl9WU3Q5aUh0MTdPd3BlWUlUeG5jUHlTQmdCV3Nmcm45eGtvdWRBSWlZaHJLaFRzbHdVZ3ZhZ1YxS"
    #           "2t2JTJCYjhJTkNWa2l3R2hCJTJCaEFkQjNDdlpmbGpaU1JPZGJ0UEFkdjV2cyUyQjlSQUlkZTNsUnU5T2pGTjFYTzNVNlAxSkF2RWs"
    #           "0RFJJWGk2QVZocXpWOTJBSGxsNHIyWnclM0QlM0Q",
    # "Sec-Ch-Ua": '"Google Chrome";v="113", "Chromium";v="113", "Not-A.Brand";v="24"',
    # "Sec-Ch-Ua-Mobile": "?0",
    # "Sec-Ch-Ua-Platform": '"Windows"',
    # "Sec-Fetch-Dest": "document",
    # "Sec-Fetch-Mode": "navigate",
    # "Sec-Fetch-Site": "none",
    # "Sec-Fetch-User": "?1",
    # "Upgrade-Insecure-Requests": "1",
    "User-Agent":
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36"
}

list_URL = [
    "https://www.metacritic.com/browse/games/score/metascore/all/ps5/filtered",
    "https://www.metacritic.com/browse/games/score/metascore/all/pc/filtered",
    "https://www.metacritic.com/browse/games/score/metascore/all/switch/filtered",
    "https://www.metacritic.com/browse/games/score/metascore/all/xbox-series-x/filtered",
    "https://www.metacritic.com/browse/games/score/metascore/all/ps4/filtered",
    "https://www.metacritic.com/browse/games/score/metascore/all/xboxone/filtered"
]
input(f"Press enter to send request for URL list. ({len(list_URL)} requests)")
choices = [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3]

# LINK LIST CREATOR
# link_list = ["https://www.metacritic.com/game/xbox-one/inside"]  # for testing comment below loop
link_list = []
for url in list_URL:
    list_page = requests.get(url, headers=headers)
    parser = BeautifulSoup(list_page.text, "html.parser")
    raw_list = parser.find_all("a", attrs="title")
    for game in raw_list:
        link_list.append("https://www.metacritic.com" + game["href"])
    # DELAY
    print("\nLoading link list..\n")  # Please don't block me
    time.sleep(random.choice(choices))  # TODO reduce when FINAL
print(link_list)

# PARSES HTML FOR FUTURE COLUMNS
debug_mode = True  # TODO remove when FINAL
if debug_mode is True:
    input(f"Press enter to initiate link list loop. ({len(link_list)} requests) **DEBUG MODE ON**")
else:
    input(f"Press enter to initiate link list loop. ({len(link_list)} requests) **DEBUG MODE OFF**")
    input(f"!!WARNING!! DEBUG MODE IS OFF. ARE YOU SURE?")

# CREATE HEADERS
csv_headers = [
    {"title": "",
     "developer": "",
     "platform": "",
     "release_date": "",
     "critic_score": "",
     "number_of_critics": "",
     "user_score": "",
     "number_of_users": "",
     "player_mode": "",
     "esrb_rating": "",
     "genre": ""}
    ]
with open("Metacritic_Games.csv", "w", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=csv_headers[0].keys())
    writer.writeheader()

# ITERATE LINKS AND WRITE ROWS
for url in link_list:
    print(f"Index: {link_list.index(url)}")  # bookmark in case of crash
    value_page = requests.get(url, headers=headers)
    value_parser = BeautifulSoup(value_page.text, "html.parser")
    # COLUMN VARIABLES
    title = value_parser.find("h1").string
    developer = value_parser.find("span", string="Developer:").find_next_sibling().find("a").string
    platform = value_parser.find("span", attrs={"class": "platform"}).find("a").string.strip()

    release = value_parser.find("span", string="Release Date:").find_next_sibling().string  # starts as string
    datetime_release = datetime.strptime(release, "%b %d, %Y")  # Lesson learned - convert to datetime with parse
    formatted_date = datetime_release.strftime("%m/%d/%Y")  # Then format as desired # TODO account for unreleased games using datetime

    meta_score = float(value_parser.find(itemprop="ratingValue").string)
    critics = int(value_parser.find("span", string="based on").find_next().find_next("span").string.strip())
    pre_user_score = value_parser.find("div", attrs={"class": "userscore_wrap feature_userscore"})
    user_score = float(pre_user_score.find("a").find("div").string)
    users = int(pre_user_score.find_all("a")[1].string.split()[0])
    try:
        players = value_parser.find("span", string="# of players:").find_next_sibling().string
    except AttributeError:
        players = ""
        print("NULL VALUE DETECTED.")
    try:
        esrb = value_parser.find("span", string="Rating:").find_next_sibling().string
    except AttributeError:
        esrb = ""
        print("NULL VALUE DETECTED")

    raw_genre = value_parser.find("span", string="Genre(s): ").find_next_siblings()
    genres = ""
    for i in raw_genre:
        if i is raw_genre[-1]:
            genres += f"{i.string}"
        else:
            genres += f"{i.string}, "

    # WRITE CSV # TODO should I convert from DataFrame to CSV instead?
    categories = [
        {
            "title": title,
            "developer": developer,
            "platform": platform,
            "release_date": formatted_date,
            "critic_score": meta_score,
            "number_of_critics": critics,
            "user_score": user_score,
            "number_of_users": users,
            "player_mode": players,  # not all entries will have this - remove?
            "esrb_rating": esrb,
            "genre": genres  # comma escape happens automatically - noice
         }
    ]

    with open("Metacritic_Games.csv", "a", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=categories[0].keys())
        writer.writerow(categories[0])

    # DEBUG PRINTS
    print(
        f"Title: {title}\n"
        f"Developer: {developer}\n"
        f"Platform: {platform.strip()}\n"
        f"Metascore: {meta_score} with {critics} Reviews\n"
        f"User score: {user_score} with {users} Ratings\n"
        f"Release Date: {formatted_date}, Raw Date: {release}\n"
        f"Players: {players}\n"
        f"ESRB Rating: {esrb}"
    )
    genre = f"Genre(s): {genres}\n"
    print(genre)

    if debug_mode is True:  # TODO remove when FINAL
        if url == link_list[8]:
            print("**DEBUG STOP**")
            break

    # DELAY
    print("\nWaiting..\n")  # Please don't block me
    time.sleep(random.choice(choices))  # TODO reduce when FINAL

# SQL Query for table import - change path and title 'Test' as needed
# select import('C:\Users\shawn\PycharmProjects\Scrape\Metacritic_Games.csv', 'CSV', 'Test', 'UTF-8',"")

# SQL Query for column data types
# CREATE TABLE test(
#      "title" TEXT,
#      "developer" TEXT,
#      "platform" TEXT,
#      "release_date" TEXT,  # TODO not sure about this one
#      "critic_score" REAL,
#      "number_of_critics" INTEGER,
#      "user_score" REAL,
#      "number_of_users" INTEGER,
#      "player_mode" TEXT,
#      "esrb_rating" TEXT,
#      "genre" TEXT)
